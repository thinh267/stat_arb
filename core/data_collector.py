# data_collector.py
from binance.client import Client
import pandas as pd
import numpy as np
from itertools import combinations
from datetime import datetime
from config import BINANCE_API_KEY, BINANCE_API_SECRET, DAILY_TOP_N
from core.supabase_manager import SupabaseManager
from statsmodels.tsa.stattools import coint
import time
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from functools import lru_cache

# TƒÉng timeout cho Binance client
client = Client(BINANCE_API_KEY, BINANCE_API_SECRET, testnet=False)
client.timeout = 30  # TƒÉng timeout l√™n 30 gi√¢y
supabase_manager = SupabaseManager()

# Cache ƒë·ªÉ l∆∞u d·ªØ li·ªáu ƒë√£ fetch
_data_cache = {}
_cache_lock = threading.Lock()

def get_data_with_retry(symbol, interval="1h", limit=168, max_retries=3):  
    """L·∫•y d·ªØ li·ªáu v·ªõi retry mechanism v√† cache"""
    cache_key = f"{symbol}_{interval}_{limit}"
    
    # Ki·ªÉm tra cache tr∆∞·ªõc
    with _cache_lock:
        if cache_key in _data_cache:
            return _data_cache[cache_key]
    
    for attempt in range(max_retries):
        try:
            klines = client.futures_klines(symbol=symbol, interval=interval, limit=limit)
            df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df['close'] = df['close'].astype(float)
            df['volume'] = df['volume'].astype(float)
            
            # Cache k·∫øt qu·∫£
            with _cache_lock:
                _data_cache[cache_key] = df
            
            return df
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"‚ö†Ô∏è  Retry {attempt + 1}/{max_retries} for {symbol}: {e}")
                time.sleep(1)  # ƒê·ª£i 1 gi√¢y tr∆∞·ªõc khi retry
            else:
                print(f"‚ùå Failed to get data for {symbol} after {max_retries} attempts: {e}")
                return None

def get_data_via_rest_api(symbol, interval="1h", limit=168):
    try:
        # Ki·ªÉm tra cache tr∆∞·ªõc
        cache_key = f"{symbol}_rest_{interval}_{limit}"
        with _cache_lock:
            if cache_key in _data_cache:
                return _data_cache[cache_key]
        
        # L·∫•y data t·ª´ REST API
        df = get_data_with_retry(symbol, interval, limit)
        
        if df is not None and len(df) > 0:
            # Cache k·∫øt qu·∫£
            with _cache_lock:
                _data_cache[cache_key] = df
            return df
        
        return None
        
    except Exception as e:
        print(f"‚ùå Error getting data via REST API cho {symbol}: {e}")
        return None

def get_data(symbol, interval="1h", limit=168):
    """Wrapper function - s·ª≠ d·ª•ng REST API v·ªõi retry mechanism"""
    return get_data_with_retry(symbol, interval, limit)

def get_all_usdt_pairs():
    """L·∫•y t·∫•t c·∫£ c·∫∑p USDT ƒëang trading v·ªõi retry"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            exchange_info = client.futures_exchange_info()
            
            # L·ªçc ch·ªâ c·∫∑p USDT v√† ƒëang trading
            usdt_pairs = []
            for symbol in exchange_info['symbols']:
                if (symbol['symbol'].endswith('USDT') and 
                    symbol['status'] == 'TRADING' and
                    symbol['contractType'] == 'PERPETUAL'):
                    usdt_pairs.append(symbol['symbol'])
            
            print(f"üìä T√¨m th·∫•y {len(usdt_pairs)} c·∫∑p USDT")
            return usdt_pairs
            
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"‚ö†Ô∏è  Retry {attempt + 1}/{max_retries} getting pairs: {e}")
                time.sleep(2)  # Gi·∫£m delay t·ª´ 3s xu·ªëng 2s
            else:
                print(f"‚ùå Error getting pairs after {max_retries} attempts: {e}")
                return []

def calculate_usdt_volume_optimized(symbol, limit=24):
    """T√≠nh volume theo USDT cho m·ªôt c·∫∑p - t·ªëi ∆∞u h√≥a"""
    try:
        # S·ª≠ d·ª•ng cache n·∫øu c√≥
        cache_key = f"{symbol}_1h_{limit}"
        with _cache_lock:
            if cache_key in _data_cache:
                df = _data_cache[cache_key]
            else:
                # L·∫•y d·ªØ li·ªáu 24h
                klines = client.futures_klines(symbol=symbol, interval="1h", limit=limit)
                df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])
                df['volume'] = df['volume'].astype(float)
                df['close'] = df['close'].astype(float)
                
                # Cache k·∫øt qu·∫£
                _data_cache[cache_key] = df
        
        # T√≠nh volume theo USDT
        avg_volume_base = df['volume'].mean()  # S·ªë l∆∞·ª£ng coin
        avg_price = df['close'].mean()  # Gi√° trung b√¨nh
        usdt_volume = avg_volume_base * avg_price  # Volume theo USDT
        
        return {
            'symbol': symbol,
            'base_volume': avg_volume_base,
            'avg_price': avg_price,
            'usdt_volume': usdt_volume,
            'current_price': df['close'].iloc[-1]
        }
        
    except Exception as e:
        print(f"‚ùå Error calculating volume for {symbol}: {e}")
        return None

def calculate_volume_batch(pairs_batch):
    """T√≠nh volume cho m·ªôt batch pairs - parallel processing"""
    results = []
    for symbol in pairs_batch:
        data = calculate_usdt_volume_optimized(symbol)
        if data and data['usdt_volume'] > 0:
            results.append(data)
    return results

def filter_pairs_by_usdt_volume_parallel(top_percentile=50, max_workers=10):
    """L·ªçc c·∫∑p theo volume USDT v·ªõi parallel processing"""
    print(f"\nüîç T√çNH TO√ÅN VOLUME THEO USDT (PARALLEL)")
    print("=" * 50)
    
    # L·∫•y t·∫•t c·∫£ c·∫∑p USDT
    pairs = get_all_usdt_pairs()
    
    if not pairs:
        print("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c danh s√°ch c·∫∑p")
        return []
    
    print(f"üìä ƒêang t√≠nh volume cho {len(pairs)} c·∫∑p (parallel)...")
    
    # Chia pairs th√†nh batches
    batch_size = max(1, len(pairs) // max_workers)
    batches = [pairs[i:i + batch_size] for i in range(0, len(pairs), batch_size)]
    
    # Parallel processing
    volume_data = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_batch = {executor.submit(calculate_volume_batch, batch): batch for batch in batches}
        
        completed = 0
        for future in as_completed(future_to_batch):
            batch_results = future.result()
            volume_data.extend(batch_results)
            completed += 1
            print(f"üìä Ho√†n th√†nh batch {completed}/{len(batches)} ({len(volume_data)} pairs processed)")
    
    if not volume_data:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu volume")
        return []
    
    # T·∫°o DataFrame v√† s·∫Øp x·∫øp
    df = pd.DataFrame(volume_data)
    df = df.sort_values('usdt_volume', ascending=False)
    
    # T√≠nh percentile v√† l·ªçc top 50%
    volume_threshold = np.percentile(df['usdt_volume'], 100 - top_percentile)
    top_pairs = df[df['usdt_volume'] >= volume_threshold]
    
    print(f"\nüìä K·∫æT QU·∫¢ FILTER:")
    print(f"- T·ªïng c·∫∑p: {len(df)}")
    print(f"- Top {top_percentile}%: {len(top_pairs)} c·∫∑p")
    print(f"- Volume threshold: ${volume_threshold:,.2f}")
    
    # Hi·ªÉn th·ªã top 10 pairs
    print(f"\nüèÜ TOP 10 PAIRS THEO VOLUME USDT:")
    print("=" * 80)
    print(f"{'Rank':<5} {'Symbol':<12} {'Volume (USDT)':<15} {'Base Volume':<15} {'Price':<12}")
    print("-" * 80)
    
    for i, row in top_pairs.head(10).iterrows():
        rank = i + 1
        symbol = row['symbol']
        usdt_vol = f"${row['usdt_volume']:,.0f}"
        base_vol = f"{row['base_volume']:.2f}"
        price = f"${row['current_price']:.2f}"
        
        print(f"{rank:<5} {symbol:<12} {usdt_vol:<15} {base_vol:<15} {price:<12}")
    
    return top_pairs['symbol'].tolist()

def check_data_quality(symbol, min_data_points=100):
    """Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu c·ªßa m·ªôt symbol"""
    try:
        # L·∫•y d·ªØ li·ªáu v·ªõi cache
        df = get_data(symbol, interval="1h", limit=168)
        
        if df is None:
            return False, 0, "No data"
        
        # Ki·ªÉm tra s·ªë l∆∞·ª£ng data points
        if len(df) < min_data_points:
            return False, len(df), f"Insufficient data points ({len(df)} < {min_data_points})"
        
        # Ki·ªÉm tra gi√° h·∫±ng s·ªë
        if df['close'].std() == 0 or df['close'].nunique() <= 1:
            return False, len(df), "Constant price"
        
        # Ki·ªÉm tra missing values
        if df['close'].isna().any():
            return False, len(df), "Missing values"
        
        # Ki·ªÉm tra volume > 0
        if df['volume'].sum() == 0:
            return False, len(df), "No volume"
        
        return True, len(df), "OK"
        
    except Exception as e:
        return False, 0, f"Error: {str(e)}"

def check_data_quality_batch(symbols_batch, min_data_points=100):
    """Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu cho m·ªôt batch symbols"""
    results = []
    for symbol in symbols_batch:
        is_valid, data_points, reason = check_data_quality(symbol, min_data_points)
        if is_valid:
            results.append(symbol)
        else:
            print(f"‚ùå {symbol}: {reason} ({data_points} points)")
    return results

def filter_data_quality_parallel(symbols, min_data_points=100, max_workers=8):
    """L·ªçc symbols theo ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu v·ªõi parallel processing"""
    print(f"\nüîç B∆Ø·ªöC 2: KI·ªÇM TRA CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU")
    print("=" * 50)
    print(f"üìä ƒêang ki·ªÉm tra {len(symbols)} c·∫∑p (min {min_data_points} data points)...")
    
    # Chia symbols th√†nh batches
    batch_size = max(1, len(symbols) // max_workers)
    batches = [symbols[i:i + batch_size] for i in range(0, len(symbols), batch_size)]
    
    # Parallel processing
    valid_symbols = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_batch = {executor.submit(check_data_quality_batch, batch, min_data_points): batch for batch in batches}
        
        completed = 0
        for future in as_completed(future_to_batch):
            batch_results = future.result()
            valid_symbols.extend(batch_results)
            completed += 1
            print(f"üìä Ho√†n th√†nh batch {completed}/{len(batches)} ({len(valid_symbols)} valid pairs)")
    
    print(f"\nüìä K·∫æT QU·∫¢ FILTER D·ªÆ LI·ªÜU:")
    print(f"- T·ªïng c·∫∑p: {len(symbols)}")
    print(f"- C·∫∑p h·ª£p l·ªá: {len(valid_symbols)}")
    print(f"- T·ª∑ l·ªá lo·∫°i b·ªè: {((len(symbols) - len(valid_symbols)) / len(symbols) * 100):.1f}%")
    
    return valid_symbols

def calculate_correlation_cointegration(symbol1, symbol2):
    try:
        # L·∫•y d·ªØ li·ªáu gi√° s·ª≠ d·ª•ng h√†m get_data
        df1 = get_data(symbol1, interval="1h", limit=168)
        df2 = get_data(symbol2, interval="1h", limit=168)
        
        if df1 is None or df2 is None:
            return None, None, None, None, None, None
        
        if len(df1) < 100 or len(df2) < 100:
            print(f"B·ªè qua {symbol1}-{symbol2}: kh√¥ng ƒë·ªß d·ªØ li·ªáu ({len(df1)}, {len(df2)})")
            return None, None, None, None, None, None
        
        # Ki·ªÉm tra gi√° h·∫±ng s·ªë - c·∫£i thi·ªán
        if (df1['close'].std() == 0 or df2['close'].std() == 0 or 
            df1['close'].nunique() <= 1 or df2['close'].nunique() <= 1 or
            df1['close'].isna().any() or df2['close'].isna().any()):
            return None, None, None, None, None, None
        
        # T√≠nh correlation
        correlation = df1['close'].corr(df2['close'])
        
        # Early exit n·∫øu correlation qu√° th·∫•p - tƒÉng threshold
        if pd.isna(correlation) or abs(correlation) < 0.5: 
            return None, None, None, None, None, None
        
        # T√≠nh rolling correlation (7 periods)
        rolling_corr = df1['close'].rolling(7).corr(df2['close']).mean()
        
        # Ki·ªÉm tra cointegration v·ªõi try-catch
        try:
            result = coint(df1['close'], df2['close'])
            p_value = result[1]
        except Exception as coint_error:
            return None, None, None, None, None, None
        
        # Ki·ªÉm tra p_value c√≥ h·ª£p l·ªá kh√¥ng
        if pd.isna(p_value):
            return None, None, None, None, None, None
        
        # T√≠nh volatility
        vol1 = df1['close'].pct_change().std() * np.sqrt(24)  # Annualized
        vol2 = df2['close'].pct_change().std() * np.sqrt(24)
        
        return correlation, p_value, rolling_corr, vol1, vol2, None
        
    except Exception as e:
        return None, None, None, None, None, None

def analyze_pair_batch(pair_batch):
    results = []
    for symbol1, symbol2 in pair_batch:
        correlation, p_value, rolling_corr, vol1, vol2, _ = calculate_correlation_cointegration(symbol1, symbol2)
        
        # Ch·ªâ l∆∞u nh·ªØng c·∫∑p c√≥ correlation cao (>0.5) v√† cointegrated
        if (correlation is not None and p_value is not None and 
            abs(correlation) > 0.5 and p_value < 0.05):
            results.append({
                'pair1': symbol1,
                'pair2': symbol2,
                'correlation': correlation,
                'rolling_correlation': rolling_corr,
                'cointegration_p_value': p_value,
                'is_cointegrated': p_value < 0.05,
                'volatility_1': vol1,
                'volatility_2': vol2
            })
    return results

def analyze_correlation_stats(results_df):
    """Ph√¢n t√≠ch th·ªëng k√™ correlation: min, max, mean, median, std"""
    stats = {}
    if len(results_df) == 0 or 'correlation' not in results_df:
        print("No correlation data to analyze.")
        return stats
    correlations = results_df['correlation'].dropna()
    stats['count'] = len(correlations)
    stats['mean'] = correlations.mean()
    stats['median'] = correlations.median()
    stats['std'] = correlations.std()
    stats['min'] = correlations.min()
    stats['max'] = correlations.max()
    print("\n========== CORRELATION STATISTICS ==========")
    print(f"S·ªë c·∫∑p: {stats['count']}")
    print(f"Mean: {stats['mean']:.4f}")
    print(f"Median: {stats['median']:.4f}")
    print(f"Std: {stats['std']:.4f}")
    print(f"Min: {stats['min']:.4f}")
    print(f"Max: {stats['max']:.4f}")
    print("===========================================\n")

    # L∆∞u th·ªëng k√™ v√†o Supabase
    supabase_manager = SupabaseManager()
    success = supabase_manager.save_correlation_stats(stats)
    if success:
        print("‚úÖ ƒê√£ l∆∞u correlation stats v√†o Supabase th√†nh c√¥ng!")
    else:
        print("‚ùå L·ªói khi l∆∞u correlation stats v√†o Supabase")
    return stats
    

def scan_market_for_stable_pairs_optimized():
    """Scan th·ªã tr∆∞·ªùng v·ªõi t·ªëi ∆∞u h√≥a parallel processing v√† data quality filter"""
    # B∆∞·ªõc 1: L·ªçc c·∫∑p theo volume USDT (top 50%) - parallel
    print("üîç B∆Ø·ªöC 1: L·ªåC C·∫∂P THEO VOLUME USDT (PARALLEL)")
    filtered_pairs = filter_pairs_by_usdt_volume_parallel(top_percentile=50, max_workers=8)
    
    if not filtered_pairs:
        print("‚ùå Kh√¥ng c√≥ c·∫∑p n√†o sau khi l·ªçc volume")
        return []
    
    # B∆∞·ªõc 2: L·ªçc theo ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu - parallel
    print(f"\nüîç B∆Ø·ªöC 2: L·ªåC THEO CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU (PARALLEL)")
    quality_filtered_pairs = filter_data_quality_parallel(filtered_pairs, min_data_points=100, max_workers=8)
    
    if not quality_filtered_pairs:
        print("‚ùå Kh√¥ng c√≥ c·∫∑p n√†o sau khi l·ªçc ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu")
        return []
    
    # B∆∞·ªõc 3: T·∫°o combinations v√† ph√¢n t√≠ch parallel
    print(f"\nüîç B∆Ø·ªöC 3: PH√ÇN T√çCH COMBINATIONS (PARALLEL)")
    print(f"üìä ƒêang t·∫°o combinations t·ª´ {len(quality_filtered_pairs)} c·∫∑p...")
    
    pair_combinations = list(combinations(quality_filtered_pairs, 2))
    print(f"üìä T·ªïng s·ªë combinations: {len(pair_combinations)}")
    
    # Chia combinations th√†nh batches cho parallel processing
    max_workers = 6  # Gi·∫£m s·ªë workers ƒë·ªÉ tr√°nh rate limit
    batch_size = max(1, len(pair_combinations) // max_workers)
    batches = [pair_combinations[i:i + batch_size] for i in range(0, len(pair_combinations), batch_size)]
    
    # Parallel processing v·ªõi progress tracking
    results = []
    completed_batches = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_batch = {executor.submit(analyze_pair_batch, batch): batch for batch in batches}
        
        for future in as_completed(future_to_batch):
            batch_results = future.result()
            results.extend(batch_results)
            completed_batches += 1
            
            # Progress tracking
            progress = (completed_batches / len(batches)) * 100
            print(f"üìà Progress: {progress:.1f}% ({completed_batches}/{len(batches)} batches) - Found {len(results)} valid pairs")
    
    # B∆∞·ªõc 4: Ph√¢n t√≠ch k·∫øt qu·∫£
    print(f"\nüìä B∆Ø·ªöC 4: PH√ÇN T√çCH K·∫æT QU·∫¢")
    print(f"‚úÖ T√¨m th·∫•y {len(results)} c·∫∑p c√≥ correlation cao v√† cointegrated")
    
    results_df = pd.DataFrame(results)
    
    if len(results_df) > 0:
        analyze_correlation_stats(results_df)
        results_df = results_df.sort_values('correlation', ascending=False)
        
        # Ch·ªâ l·∫•y top 10 c·∫∑p
        top_10_pairs = results_df.head(10)
        print(f"\nüèÜ TOP 10 PAIRS:")
        print("=" * 80)
        print(f"{'Rank':<5} {'Pair':<20} {'Correlation':<12} {'P-Value':<12} {'Cointegrated':<12}")
        print("-" * 80)
        
        for idx, row in top_10_pairs.iterrows():
            rank = idx + 1
            pair = f"{row['pair1']}-{row['pair2']}"
            corr = f"{row['correlation']:.4f}"
            p_val = f"{row['cointegration_p_value']:.4f}"
            coint = "‚úÖ" if row['is_cointegrated'] else "‚ùå"
            
            print(f"{rank:<5} {pair:<20} {corr:<12} {p_val:<12} {coint:<12}")
        
        # L∆∞u ch·ªâ top 10 v√†o Supabase
        today = datetime.now().date()
        pairs_data = []
        for idx, row in top_10_pairs.iterrows():
            pairs_data.append({
                'date': str(today),
                'pair1': row['pair1'],
                'pair2': row['pair2'],
                'correlation': float(row['correlation']),
                'rolling_correlation': float(row['rolling_correlation']) if not pd.isna(row['rolling_correlation']) else None,
                'cointegration_p_value': float(row['cointegration_p_value']),
                'is_cointegrated': bool(row['is_cointegrated']),
                'volatility_1': float(row['volatility_1']) if not pd.isna(row['volatility_1']) else None,
                'volatility_2': float(row['volatility_2']) if not pd.isna(row['volatility_2']) else None,
                'rank': int(idx) + 1
            })
        
        # L∆∞u daily pairs v√† l·∫•y l·∫°i ƒë·ªÉ c√≥ ID
        saved_pairs = supabase_manager.save_daily_pairs(pairs_data)
        print(f"‚úÖ ƒê√£ l∆∞u top 10 c·∫∑p v√†o database")

        # L∆∞u hourly ranking v·ªõi ID tr·ª±c ti·∫øp t·ª´ saved_pairs
        ranking_data = []
        for idx, pair in enumerate(saved_pairs):
            correlation, p_value, rolling_corr, vol1, vol2, _ = calculate_correlation_cointegration(pair['pair1'], pair['pair2'])
            if correlation is not None:
                ranking_data.append({
                    'timestamp': datetime.now().isoformat(),
                    'pair_id': pair['id'],  # S·ª≠ d·ª•ng ID tr·ª±c ti·∫øp t·ª´ saved_pairs
                    'current_rank': idx + 1,
                    'current_correlation': float(correlation),
                    'rolling_correlation': float(rolling_corr) if rolling_corr is not None else None,
                    'volatility_1': float(vol1) if vol1 is not None else None,
                    'volatility_2': float(vol2) if vol2 is not None else None
                })
                print(f"‚úÖ Hourly ranking: {pair['pair1']}-{pair['pair2']}: pair_id {pair['id']}")
        
        if ranking_data:
            supabase_manager.update_hourly_ranking(ranking_data)
            print(f"‚úÖ ƒê√£ l∆∞u hourly ranking v·ªõi {len(ranking_data)} pairs")
        else:
            print("‚ö†Ô∏è Kh√¥ng c√≥ hourly ranking data ƒë·ªÉ l∆∞u")

        return pairs_data
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y c·∫∑p n√†o h·ª£p l·ªá")
        return []

def reorder_pairs_by_correlation():
    # Get today's pairs
    top_pairs = supabase_manager.get_current_top_n(DAILY_TOP_N)
    ranking_data = []
    
    print(f"üîÑ Reordering {len(top_pairs)} pairs by correlation...")
    
    # T√≠nh correlation m·ªõi cho t·∫•t c·∫£ pairs
    pairs_with_correlation = []
    for pair in top_pairs:
        correlation, p_value, rolling_corr, vol1, vol2, _ = calculate_correlation_cointegration(pair['pair1'], pair['pair2'])
        if correlation is not None:
            pairs_with_correlation.append({
                'pair': pair,
                'correlation': correlation,
                'rolling_correlation': rolling_corr,
                'volatility_1': vol1,
                'volatility_2': vol2
            })
            print(f"üìä {pair['pair1']}-{pair['pair2']}: corr {correlation:.4f}")
        else:
            print(f"‚ö†Ô∏è B·ªè qua {pair['pair1']}-{pair['pair2']}: kh√¥ng t√≠nh ƒë∆∞·ª£c correlation")
    
    # S·∫Øp x·∫øp theo correlation m·ªõi (cao ‚Üí th·∫•p)
    pairs_with_correlation.sort(key=lambda x: x['correlation'], reverse=True)
    
    # T·∫°o ranking data v·ªõi rank ƒë√∫ng
    for idx, pair_data in enumerate(pairs_with_correlation):
        pair = pair_data['pair']
        ranking_data.append({
            'timestamp': datetime.now().isoformat(),
            'pair_id': pair['id'],
            'current_rank': idx + 1,  # Rank theo correlation m·ªõi
            'current_correlation': float(pair_data['correlation']),
            'rolling_correlation': float(pair_data['rolling_correlation']) if pair_data['rolling_correlation'] is not None else None,
            'volatility_1': float(pair_data['volatility_1']) if pair_data['volatility_1'] is not None else None,
            'volatility_2': float(pair_data['volatility_2']) if pair_data['volatility_2'] is not None else None
        })
        print(f"‚úÖ {pair['pair1']}-{pair['pair2']}: rank {idx+1}, corr {pair_data['correlation']:.4f}, pair_id {pair['id']}")
    
    # C·∫≠p nh·∫≠t v√†o database
    if ranking_data:
        supabase_manager.update_hourly_ranking(ranking_data)
        print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t hourly ranking v·ªõi {len(ranking_data)} pairs")
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ ranking data ƒë·ªÉ c·∫≠p nh·∫≠t")
    
    return ranking_data

# Alias cho backward compatibility
scan_market_for_stable_pairs = scan_market_for_stable_pairs_optimized 